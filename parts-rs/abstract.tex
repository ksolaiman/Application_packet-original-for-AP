\noindent \semisection*{Abstract}
Real-world use cases in data-centric disciplines often have an unprecedented influx of unstructured and noisy data from multiple sources and modalities. Extraction of meaningful information from such overwhelming and dirty datasets requires achieving the complementary goals of computational efficiency and contextually-aware multimodal information recommendation. My research aims to understand how these goals can be achieved by designing interactive algorithms and frameworks for scalable and efficient systems that can adapt to changing data patterns and operate under resource constraints in open-world. Such a design poses a few significant challenges: (1) resource-aware data management (involving processes such as data ingestion and delivery, knowledge base formation, and multimodal feature extraction): I will present a novel framework, SKOD, a scalable and on-demand Situational Knowledge Query Engine, to deliver multimodal content to appropriate users through triggers by continuously processing and building a multi-modal relational knowledge base using SQL queries entity-centric concept extraction (such as person – gender, race, clothes, color of clothes) from multiple modalities; (2) label-independent data integration: I will introduce FemmIR, a weakly supervised multimodal retrieval model using Graph Edit Distance to tackle the lack of training samples that can integrate new modalities and produce rankings of exactly and approximately relevant data. Additionally, I will introduce WesJeM, another weakly supervised model that performs a feature-level late fusion using the entity-centric high-level features with semantic concepts and by building a common multimodal representation space; (3) adaption to open-world novelties: After introducing changing data patterns and characterizing ‘novelty’ in multimodal information retrieval, I would discuss how WesJeM tackles novel situations including zero-shot learning. Further, I would explain how we used a novelty generation framework to augment visual modality datasets in a domain-agnostic and budget-efficient manner and improved the object-detection models’ adaptation capabilities. Finally, I will discuss how we learned and leveraged inherent data complexity of visual modalities in different learning environments to identify the difficulty of handling novelty. Such an Intelligent Multimodal and Situation-aware Recommendation Engine for real-world environments pave the path for the next generation information access for many open-ended application domains. I will also show SKOD’s positive results on open-world applications, such as finding missing persons, data discovery, and urban information systems.
 